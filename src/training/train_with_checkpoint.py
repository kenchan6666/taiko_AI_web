from transformers import Trainer, TrainingArguments, AutoModelForCausalLM, AutoTokenizer
import datasets
import torch
import os

# **ğŸ“Œ 1. åŠ è½½æ¨¡å‹å’Œ tokenizer**
model_name = "gpt2"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# **ğŸ“Œ 2. åŠ è½½æ•°æ®é›†**
train_dataset = datasets.load_dataset("json", data_files="data/taiko_dataset.jsonl", split="train")

# **ğŸ“Œ 3. Tokenize æ•°æ®ï¼ˆå¼€å¯å¤šçº¿ç¨‹ï¼‰**
def tokenize_function(examples):
    return tokenizer(
        examples["notes"],  # **ä½¿ç”¨ "notes" ä½œä¸ºè¾“å…¥å­—æ®µ**
        truncation=True,
        padding="max_length",
        max_length=512
    )

train_dataset = train_dataset.map(tokenize_function, batched=True, num_proc=4)  # **å¼€å¯ 4 çº¿ç¨‹åŠ é€Ÿ**

# **ğŸ“Œ 4. è®­ç»ƒå‚æ•°**
training_args = TrainingArguments(
    output_dir="./model/taiko_model_trained",
    evaluation_strategy="steps",  # **æ¯ save_steps è¯„ä¼°ä¸€æ¬¡**
    save_strategy="steps",  # **æ¯éš” X æ­¥ä¿å­˜æ¨¡å‹**
    save_steps=500,  # **æ¯ 500 æ­¥ä¿å­˜**
    save_total_limit=3,  # **æœ€å¤šä¿ç•™ 3 ä¸ª checkpoint**
    per_device_train_batch_size=4,  # **å¢å¤§ batch sizeï¼ŒåŠ å¿«è®­ç»ƒ**
    num_train_epochs=3,
    fp16=torch.cuda.is_available(),  # **å¦‚æœ GPU æ”¯æŒ FP16ï¼Œåˆ™å¯ç”¨**
    dataloader_num_workers=4,  # **å¤šçº¿ç¨‹æ•°æ®åŠ è½½**
    report_to="none"  # **å…³é—­ Wandb / Tensorboard**
)

# **ğŸ“Œ 5. è®­ç»ƒå™¨**
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    tokenizer=tokenizer
)

# **ğŸ“Œ 6. æ£€æŸ¥æ˜¯å¦æœ‰ Checkpoint**
checkpoint_path = "./model/taiko_model_trained/checkpoint-latest"
if os.path.exists(checkpoint_path):
    print(f"ğŸ”„ ç»§ç»­ä» {checkpoint_path} è®­ç»ƒ...")
    trainer.train(resume_from_checkpoint=checkpoint_path)
else:
    print("ğŸš€ å¼€å§‹æ–°çš„è®­ç»ƒ...")
    trainer.train()
